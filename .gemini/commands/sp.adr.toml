description = "Review planning artifacts for architecturally significant decisions for the Book RAG project and create ADRs."

prompt = """
---
description: Review planning artifacts for architecturally significant decisions for the Book RAG project and create ADRs.
---

# COMMAND: Analyze planning artifacts and document architecturally significant decisions as ADRs for the Book RAG project

## CONTEXT

The user has completed feature planning for the Book RAG project and needs to:

- Identify architecturally significant technical decisions from plan.md.
- Document these decisions as Architecture Decision Records (ADRs).
- Ensure team alignment on the technical approach for the Docusaurus, Gemini, Neon, and Qdrant stack.
- Create a permanent, reviewable record of why decisions were made.

Architecture Decision Records capture decisions that:

- Impact how engineers write or structure software within the Book RAG project.
- Have notable tradeoffs or alternatives (e.g., choosing a different vector DB over Qdrant).
- Will likely be questioned or revisited later.

**User's additional input:**

$ARGUMENTS

## YOUR ROLE

Act as a senior software architect with expertise in:

- RAG architectures, Docusaurus, and AI chat integrations.
- System design patterns and tradeoffs for documentation platforms.
- Enterprise architecture documentation.
- Risk assessment for data and AI systems.

## OUTPUT STRUCTURE (with quick flywheel hooks)

Execute this workflow in 6 sequential steps. At Steps 2 and 4, apply lightweight Analyze‚ÜíMeasure checks:
 - Analyze: Identify likely failure modes, specifically:
     - Over-granular ADRs: ADRs that document trivial decisions (e.g., component naming).
     - Missing alternatives: ADRs that do not list at least one alternative approach considered (e.g., using Pinecone instead of Qdrant).
 - Measure: Apply the following checklist grader (PASS only if all are met):
     - The ADR documents a decision that impacts multiple components (e.g., the RAG pipeline).
     - The ADR explicitly lists at least one alternative approach with rationale.
     - The ADR includes clear pros and cons for the chosen approach and alternatives.
     - The ADR is concise but sufficiently detailed for future reference.

## Step 1: Load Planning Context

Run `.specify/scripts/powershell/check-prerequisites.ps1 -Json` from repo root and parse JSON for FEATURE_DIR and AVAILABLE_DOCS.

Derive absolute paths:

- PLAN = FEATURE_DIR/plan.md (REQUIRED - abort if missing with "Run /sp.plan first")
- RESEARCH = FEATURE_DIR/research.md (if exists)
- DATA_MODEL = FEATURE_DIR/data-model.md (if exists)
- CONTRACTS_DIR = FEATURE_DIR/contracts/ (if exists)

## Step 2: Extract Architectural Decisions (Analyze)

Load plan.md and available artifacts. Extract architecturally significant decisions as **decision clusters**:

**‚úÖ GOOD (Clustered for Book RAG):**

- "RAG Pipeline Architecture" (Gemini Model + Qdrant + Neon DB integration)
- "Authentication Strategy" (Lucia Auth/Better Auth integration with Docusaurus)
- "Content Ingestion and Embedding" (How book content is chunked, embedded, and stored in Qdrant)

**‚ùå BAD (Over-granular):**

- Separate ADRs for Gemini, Qdrant, and Neon.
- An ADR for a single React component in Docusaurus.

**Clustering Rules:**

- Group technologies that work together to deliver a core feature (like the RAG pipeline).
- Separate decisions that can evolve independently (e.g., frontend styling vs. backend database choice).

For each decision cluster, note: what was decided, why, where in docs.

## Step 3: Check Existing ADRs

Scan `history/adr/` directory. For each extracted decision:

- If covered by existing ADR ‚Üí note reference.
- If conflicts with existing ADR ‚Üí flag conflict.
- If not covered ‚Üí mark as ADR candidate.

## Step 4: Apply Significance Test (Measure)

For each ADR candidate, test:

- Does it impact how engineers write or structure the Docusaurus site or the backend AI services?
- Are there notable tradeoffs (e.g., cost vs. performance of Gemini models)?
- Will it be questioned or revisited later?

Only proceed with ADRs that pass ALL three tests.

## Step 5: Create ADRs (Improve)

For each qualifying decision cluster:

1. Generate a concise title reflecting the cluster (e.g., "AI Chat and RAG Pipeline Architecture").
2. Run `create-adr.sh "<title>"` from repo root.
3. Parse JSON response for `adr_path` and `adr_id`.
4. Read the created file (contains template with {{PLACEHOLDERS}}).
5. Fill ALL placeholders:
   - `{{TITLE}}` = decision cluster title
   - `{{STATUS}}` = "Proposed" or "Accepted"
   - `{{DATE}}` = today (YYYY-MM-DD)
   - `{{CONTEXT}}` = The need for an interactive, AI-powered book experience.
   - `{{DECISION}}` = The chosen stack (e.g., "Gemini for generation, Qdrant for vector search, Neon for relational data").
   - `{{CONSEQUENCES}}` = Outcomes, tradeoffs (e.g., vendor lock-in, performance characteristics).
   - `{{ALTERNATIVES}}` = Alternative clusters (e.g., "OpenAI models + Pinecone").
   - `{{REFERENCES}}` = plan.md, research.md, data-model.md
6. Save file.

## Step 6: Report Completion

Output:

`‚úÖ ADR Review Complete - Created N ADRs, referenced M existing`

List created ADRs with ID and title.

If conflicts detected:

`‚ö†Ô∏è Conflicts with existing ADRs [IDs]. Review and update outdated decisions or revise plan.`

If create-adr.sh fails: Report script error and skip that ADR.

## FORMATTING REQUIREMENTS

Present results in this exact structure:

`‚úÖ ADR Review Complete
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

üìã Created ADRs: {count}
   - ADR-{id}: {title}
   - ADR-{id}: {title}

üìö Referenced Existing: {count}
   - ADR-{id}: {title}

‚ö†Ô∏è  Conflicts Detected: {count}
   - ADR-{id}: {conflict description}

Next Steps:
‚Üí Resolve conflicts before proceeding to /sp.tasks
‚Üí Review created ADRs with team
‚Üí Update plan.md if needed`

## ERROR HANDLING

If plan.md missing:

- Display: "‚ùå Error: plan.md not found. Run /sp.plan first to generate planning artifacts."
- Exit gracefully without creating any ADRs.

If create-adr.sh fails:

- Display exact error message.
- Skip that ADR and continue with others.
- Report partial completion at end.

## TONE

Be thorough, analytical, and decision-focused, with an emphasis on the "why" behind decisions for the Book RAG project's long-term success.

---

As the main request completes, you MUST create and complete a PHR (Prompt History Record).
"""