"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[5847],{2399:(n,e,r)=>{r.r(e),r.d(e,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>s,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"physical-ai-book/chapter6/vla-foundations","title":"6.1 Vision-Language-Action (VLA) Foundations","description":"Learning Objectives","source":"@site/docs/physical-ai-book/chapter6/vla-foundations.md","sourceDirName":"physical-ai-book/chapter6","slug":"/physical-ai-book/chapter6/vla-foundations","permalink":"/RAGai/docs/physical-ai-book/chapter6/vla-foundations","draft":false,"unlisted":false,"editUrl":"https://github.com/muhammad-anas35/RAGai/tree/main/docs/physical-ai-book/chapter6/vla-foundations.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"6.2 Multimodal Integration","permalink":"/RAGai/docs/physical-ai-book/chapter6/multimodal-integration"}}');var t=r(4848),o=r(8453);const s={},a="6.1 Vision-Language-Action (VLA) Foundations",l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction",id:"introduction",level:2},{value:"VLA Architecture",id:"vla-architecture",level:2},{value:"RT-2 Example",id:"rt-2-example",level:2},{value:"\u03c00 (Pi-Zero)",id:"\u03c00-pi-zero",level:2},{value:"Fine-Tuning VLAs",id:"fine-tuning-vlas",level:2},{value:"Key Takeaways",id:"key-takeaways",level:2}];function d(n){const e={a:"a",br:"br",code:"code",h1:"h1",h2:"h2",header:"header",hr:"hr",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...n.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.header,{children:(0,t.jsx)(e.h1,{id:"61-vision-language-action-vla-foundations",children:"6.1 Vision-Language-Action (VLA) Foundations"})}),"\n",(0,t.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Understand Vision-Language-Action models"}),"\n",(0,t.jsx)(e.li,{children:"Explore foundation models for robotics (RT-2, PaLM-E, \u03c00)"}),"\n",(0,t.jsx)(e.li,{children:"Implement VLA inference for robot control"}),"\n",(0,t.jsx)(e.li,{children:"Fine-tune VLAs for custom tasks"}),"\n"]}),"\n",(0,t.jsx)(e.hr,{}),"\n",(0,t.jsx)(e.h2,{id:"introduction",children:"Introduction"}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Vision-Language-Action (VLA) models"})," are foundation models that:"]}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"See"}),": Process visual input (cameras)"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Understand"}),": Interpret natural language commands"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Act"}),": Generate robot actions"]}),"\n"]}),"\n",(0,t.jsxs)(e.p,{children:["Examples: ",(0,t.jsx)(e.strong,{children:"RT-2"})," (Google), ",(0,t.jsx)(e.strong,{children:"PaLM-E"})," (Google), ",(0,t.jsx)(e.strong,{children:"\u03c00"})," (Physical Intelligence)"]}),"\n",(0,t.jsx)(e.hr,{}),"\n",(0,t.jsx)(e.h2,{id:"vla-architecture",children:"VLA Architecture"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{children:'\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502   Camera    \u2502 \u2500\u2500\u25b6 Vision Encoder (ViT)\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518              \u2502\r\n                             \u25bc\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502  "Pick up   \u2502 \u2500\u2500\u25b6    \u2502   LLM    \u2502 \u2500\u2500\u25b6 Action Tokens\r\n\u2502  the cup"   \u2502        \u2502 (7B-540B)\u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n                             \u2502\r\n                             \u25bc\r\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n                    \u2502 Action Decoder \u2502\r\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n                             \u2502\r\n                             \u25bc\r\n                    [x, y, z, gripper]\n'})}),"\n",(0,t.jsx)(e.hr,{}),"\n",(0,t.jsx)(e.h2,{id:"rt-2-example",children:"RT-2 Example"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'from transformers import RT2ForConditionalGeneration, AutoProcessor\r\n\r\n# Load RT-2 model\r\nmodel = RT2ForConditionalGeneration.from_pretrained("google/rt-2-base")\r\nprocessor = AutoProcessor.from_pretrained("google/rt-2-base")\r\n\r\n# Get observation\r\nimage = camera.get_rgb()\r\ntext = "pick up the red block"\r\n\r\n# Inference\r\ninputs = processor(text=text, images=image, return_tensors="pt")\r\noutputs = model.generate(**inputs)\r\n\r\n# Decode action\r\naction = processor.decode(outputs[0])\r\n# action = {"x": 0.5, "y": 0.3, "z": 0.2, "gripper": 1.0}\n'})}),"\n",(0,t.jsx)(e.hr,{}),"\n",(0,t.jsx)(e.h2,{id:"\u03c00-pi-zero",children:"\u03c00 (Pi-Zero)"}),"\n",(0,t.jsxs)(e.p,{children:["Physical Intelligence's ",(0,t.jsx)(e.strong,{children:"\u03c00"})," is a generalist robot policy:"]}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:["Trained on ",(0,t.jsx)(e.strong,{children:"10,000+ hours"})," of robot data"]}),"\n",(0,t.jsxs)(e.li,{children:["Handles ",(0,t.jsx)(e.strong,{children:"diverse tasks"})," (folding, assembly, cleaning)"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Zero-shot"})," generalization to new objects"]}),"\n"]}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'# \u03c00 API (hypothetical)\r\nfrom pi_zero import Pi0Policy\r\n\r\npolicy = Pi0Policy.from_pretrained("pi0-1.5b")\r\n\r\n# Natural language command\r\naction = policy.predict(\r\n    image=camera.get_rgb(),\r\n    instruction="fold the shirt",\r\n    robot_state=robot.get_state()\r\n)\r\n\r\nrobot.execute(action)\n'})}),"\n",(0,t.jsx)(e.hr,{}),"\n",(0,t.jsx)(e.h2,{id:"fine-tuning-vlas",children:"Fine-Tuning VLAs"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'from transformers import Trainer, TrainingArguments\r\n\r\n# Prepare dataset\r\ndataset = load_robot_dataset("custom_tasks")\r\n\r\n# Training arguments\r\ntraining_args = TrainingArguments(\r\n    output_dir="./rt2_finetuned",\r\n    num_train_epochs=10,\r\n    per_device_train_batch_size=8,\r\n    learning_rate=1e-5,\r\n)\r\n\r\n# Fine-tune\r\ntrainer = Trainer(\r\n    model=model,\r\n    args=training_args,\r\n    train_dataset=dataset,\r\n)\r\n\r\ntrainer.train()\n'})}),"\n",(0,t.jsx)(e.hr,{}),"\n",(0,t.jsx)(e.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,t.jsxs)(e.p,{children:["\u2705 ",(0,t.jsx)(e.strong,{children:"VLAs"})," combine vision, language, and action in one model",(0,t.jsx)(e.br,{}),"\n","\u2705 ",(0,t.jsx)(e.strong,{children:"Foundation models"})," (RT-2, \u03c00) enable zero-shot robot control",(0,t.jsx)(e.br,{}),"\n","\u2705 ",(0,t.jsx)(e.strong,{children:"Fine-tuning"})," adapts VLAs to custom tasks",(0,t.jsx)(e.br,{}),"\n","\u2705 ",(0,t.jsx)(e.strong,{children:"Natural language"})," makes robots accessible to non-experts"]}),"\n",(0,t.jsx)(e.hr,{}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Previous Chapter"}),": ",(0,t.jsx)(e.a,{href:"/RAGai/docs/physical-ai-book/chapter5/real-robot-deployment",children:"\u2190 Chapter 5: Humanoid Development"}),(0,t.jsx)(e.br,{}),"\n",(0,t.jsx)(e.strong,{children:"Next Section"}),": ",(0,t.jsx)(e.a,{href:"/RAGai/docs/physical-ai-book/chapter6/multimodal-integration",children:"6.2 Multimodal Integration \u2192"})]})]})}function h(n={}){const{wrapper:e}={...(0,o.R)(),...n.components};return e?(0,t.jsx)(e,{...n,children:(0,t.jsx)(d,{...n})}):d(n)}},8453:(n,e,r)=>{r.d(e,{R:()=>s,x:()=>a});var i=r(6540);const t={},o=i.createContext(t);function s(n){const e=i.useContext(o);return i.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function a(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(t):n.components||t:s(n.components),i.createElement(o.Provider,{value:e},n.children)}}}]);