"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[6829],{1910:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>a,contentTitle:()=>o,default:()=>h,frontMatter:()=>t,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"physical-ai-book/intro/sensor-systems","title":"1.4 Sensor Systems Overview","description":"Learning Objectives","source":"@site/docs/physical-ai-book/01-intro/sensor-systems.md","sourceDirName":"physical-ai-book/01-intro","slug":"/physical-ai-book/intro/sensor-systems","permalink":"/docs/physical-ai-book/intro/sensor-systems","draft":false,"unlisted":false,"editUrl":"https://github.com/muhammad-anas35/RAGai/tree/main/docs/physical-ai-book/01-intro/sensor-systems.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"1.3 Humanoid Robotics Landscape","permalink":"/docs/physical-ai-book/intro/humanoid-landscape"},"next":{"title":"Chapter 2 Creation Workflow - Multi-Agent Coordination","permalink":"/docs/physical-ai-book/chapter2/CHAPTER2_WORKFLOW"}}');var r=s(4848),l=s(8453);const t={},o="1.4 Sensor Systems Overview",a={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction",id:"introduction",level:2},{value:"1. LiDAR (Light Detection and Ranging)",id:"1-lidar-light-detection-and-ranging",level:2},{value:"How LiDAR Works",id:"how-lidar-works",level:3},{value:"Types of LiDAR",id:"types-of-lidar",level:3},{value:"Mechanical Spinning LiDAR",id:"mechanical-spinning-lidar",level:4},{value:"Solid-State LiDAR",id:"solid-state-lidar",level:4},{value:"Digital Spinning LiDAR",id:"digital-spinning-lidar",level:4},{value:"LiDAR Comparison Table",id:"lidar-comparison-table",level:3},{value:"2. RGB-D Cameras (Depth Cameras)",id:"2-rgb-d-cameras-depth-cameras",level:2},{value:"Intel RealSense D435i",id:"intel-realsense-d435i",level:3},{value:"How Stereoscopic Depth Works",id:"how-stereoscopic-depth-works",level:3},{value:"3. Inertial Measurement Units (IMUs)",id:"3-inertial-measurement-units-imus",level:2},{value:"IMU Components",id:"imu-components",level:3},{value:"BNO055 vs. MPU6050",id:"bno055-vs-mpu6050",level:3},{value:"Why IMUs Matter for Humanoids",id:"why-imus-matter-for-humanoids",level:3},{value:"4. Force/Torque Sensors",id:"4-forcetorque-sensors",level:2},{value:"Applications",id:"applications",level:3},{value:"Example: ATI Mini40",id:"example-ati-mini40",level:3},{value:"5. Sensor Fusion",id:"5-sensor-fusion",level:2},{value:"Example: Visual-Inertial Odometry (VIO)",id:"example-visual-inertial-odometry-vio",level:3},{value:"Common Fusion Techniques",id:"common-fusion-techniques",level:3},{value:"ROS 2 Code Example: Reading Sensor Data",id:"ros-2-code-example-reading-sensor-data",level:2},{value:"Prerequisites",id:"prerequisites",level:3},{value:"Code: <code>imu_subscriber.py</code>",id:"code-imu_subscriberpy",level:3},{value:"How to Run",id:"how-to-run",level:3},{value:"Code Explanation",id:"code-explanation",level:3},{value:"Key Takeaways",id:"key-takeaways",level:2},{value:"Reflection Questions",id:"reflection-questions",level:2},{value:"Further Reading",id:"further-reading",level:2},{value:"Chapter 1 Summary",id:"chapter-1-summary",level:2}];function d(e){const n={a:"a",br:"br",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,l.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"14-sensor-systems-overview",children:"1.4 Sensor Systems Overview"})}),"\n",(0,r.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,r.jsx)(n.p,{children:"By the end of this section, you will be able to:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Identify the major sensor types used in humanoid robotics"}),"\n",(0,r.jsx)(n.li,{children:"Understand the specifications and applications of LiDAR, RGB-D cameras, and IMUs"}),"\n",(0,r.jsx)(n.li,{children:"Compare different sensor technologies and their trade-offs"}),"\n",(0,r.jsx)(n.li,{children:"Recognize the role of sensor fusion in Physical AI systems"}),"\n",(0,r.jsx)(n.li,{children:"Write basic ROS 2 code to subscribe to sensor data"}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,r.jsxs)(n.p,{children:['If embodied AI is the "brain" of a humanoid robot, then ',(0,r.jsx)(n.strong,{children:"sensors are its eyes, ears, and sense of touch"}),". Without sensors, a robot is blind to its environment\u2014unable to navigate, manipulate objects, or respond to the world around it."]}),"\n",(0,r.jsx)(n.p,{children:"This section introduces the core sensor technologies that enable Physical AI, with a focus on the sensors you'll encounter in this course:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"LiDAR"}),": For 3D spatial mapping"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"RGB-D Cameras"}),": For visual perception and depth sensing"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"IMUs"}),": For orientation and motion tracking"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Force/Torque Sensors"}),": For manipulation feedback"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"We'll also write our first ROS 2 program to read sensor data\u2014a preview of Chapter 2."}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"1-lidar-light-detection-and-ranging",children:"1. LiDAR (Light Detection and Ranging)"}),"\n",(0,r.jsx)(n.p,{children:"LiDAR sensors emit laser pulses and measure the time it takes for reflections to return, creating precise 3D point clouds of the environment."}),"\n",(0,r.jsx)(n.h3,{id:"how-lidar-works",children:"How LiDAR Works"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Emit"}),": Laser pulses are sent out"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Reflect"}),": Pulses bounce off objects"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Measure"}),": Time-of-flight (ToF) is calculated"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Compute"}),": Distance = (Speed of Light \xd7 Time) / 2"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"types-of-lidar",children:"Types of LiDAR"}),"\n",(0,r.jsx)(n.h4,{id:"mechanical-spinning-lidar",children:"Mechanical Spinning LiDAR"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Example"}),": Velodyne VLP-16 Puck"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Mechanism"}),": Multiple lasers rotate 360\xb0 on a spinning platform"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Pros"}),": Complete surround view, proven technology"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Cons"}),": Moving parts (wear and tear), bulky, expensive ($4,000-$8,000)"]}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"solid-state-lidar",children:"Solid-State LiDAR"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Example"}),": Livox Mid-360"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Mechanism"}),": No moving parts; uses MEMS mirrors or phased arrays"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Pros"}),": Compact, durable, lower cost ($500-$1,500)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Cons"}),": Limited field of view (often < 360\xb0)"]}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"digital-spinning-lidar",children:"Digital Spinning LiDAR"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Example"}),": Ouster OS1"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Mechanism"}),": Electronically controlled laser firing with structured data output"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Pros"}),": High resolution, uniform point density, rugged (IP68/IP69K)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Cons"}),": More expensive than solid-state"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"lidar-comparison-table",children:"LiDAR Comparison Table"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Sensor"}),(0,r.jsx)(n.th,{children:"Type"}),(0,r.jsx)(n.th,{children:"Range"}),(0,r.jsx)(n.th,{children:"FOV"}),(0,r.jsx)(n.th,{children:"Points/sec"}),(0,r.jsx)(n.th,{children:"Price"}),(0,r.jsx)(n.th,{children:"Use Case"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Velodyne VLP-16"})}),(0,r.jsx)(n.td,{children:"Mechanical Spinning"}),(0,r.jsx)(n.td,{children:"100m"}),(0,r.jsx)(n.td,{children:"360\xb0 \xd7 30\xb0"}),(0,r.jsx)(n.td,{children:"300,000"}),(0,r.jsx)(n.td,{children:"~$4,000"}),(0,r.jsx)(n.td,{children:"Autonomous vehicles, outdoor mapping"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Ouster OS1-64"})}),(0,r.jsx)(n.td,{children:"Digital Spinning"}),(0,r.jsx)(n.td,{children:"120m"}),(0,r.jsx)(n.td,{children:"360\xb0 \xd7 45\xb0"}),(0,r.jsx)(n.td,{children:"1,310,720"}),(0,r.jsx)(n.td,{children:"~$12,000"}),(0,r.jsx)(n.td,{children:"High-res mapping, industrial"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Livox Mid-360"})}),(0,r.jsx)(n.td,{children:"Solid-State (MEMS)"}),(0,r.jsx)(n.td,{children:"40m"}),(0,r.jsx)(n.td,{children:"360\xb0 \xd7 59\xb0"}),(0,r.jsx)(n.td,{children:"200,000"}),(0,r.jsx)(n.td,{children:"~$500"}),(0,r.jsx)(n.td,{children:"Mobile robots, drones, SLAM"})]})]})]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"For this course"}),": The ",(0,r.jsx)(n.strong,{children:"Livox Mid-360"})," (used in Unitree G1) represents the best balance of cost and capability for educational robotics."]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"2-rgb-d-cameras-depth-cameras",children:"2. RGB-D Cameras (Depth Cameras)"}),"\n",(0,r.jsx)(n.p,{children:"RGB-D cameras combine standard color imaging with depth sensing, providing both visual appearance and 3D structure."}),"\n",(0,r.jsx)(n.h3,{id:"intel-realsense-d435i",children:"Intel RealSense D435i"}),"\n",(0,r.jsxs)(n.p,{children:["The ",(0,r.jsx)(n.strong,{children:"RealSense D435i"})," is the industry standard for robotics education and research."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Key Specifications"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Depth Technology"}),": Stereoscopic (dual global shutter cameras)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Depth Range"}),": 0.3m to 3m (ideal), up to 10m (max)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Depth Accuracy"}),": <2% at 2m (~20mm error)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Depth Resolution"}),": Up to 1280\xd7720 @ 90 FPS"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"RGB Resolution"}),": 1920\xd71080 @ 30 FPS"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Field of View"}),": 87\xb0 \xd7 58\xb0 (H \xd7 V)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"IMU"}),": Integrated BMI055 (6-DoF)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Interface"}),": USB-C 3.1"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Price"}),": ~$349"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Why It's Popular"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Global Shutter"}),": No motion blur (critical for moving robots)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Integrated IMU"}),": Enables visual-inertial odometry (VIO)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"ROS 2 Support"}),": Official ",(0,r.jsx)(n.code,{children:"realsense-ros"})," package"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Compact"}),": 90mm \xd7 25mm \xd7 25mm"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"how-stereoscopic-depth-works",children:"How Stereoscopic Depth Works"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Two cameras capture the same scene from slightly different angles"}),"\n",(0,r.jsx)(n.li,{children:"Software identifies matching features in both images"}),"\n",(0,r.jsx)(n.li,{children:"Triangulation calculates depth based on disparity (pixel offset)"}),"\n",(0,r.jsx)(n.li,{children:"Result: A depth map where each pixel has a distance value"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Limitation"}),": Struggles with textureless surfaces (white walls, glass) because there are no features to match."]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"3-inertial-measurement-units-imus",children:"3. Inertial Measurement Units (IMUs)"}),"\n",(0,r.jsx)(n.p,{children:"IMUs measure orientation, angular velocity, and linear acceleration\u2014essential for balance and navigation."}),"\n",(0,r.jsx)(n.h3,{id:"imu-components",children:"IMU Components"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Accelerometer"}),": Measures linear acceleration (m/s\xb2) in 3 axes"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Gyroscope"}),": Measures angular velocity (\xb0/s or rad/s) in 3 axes"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Magnetometer"})," (optional): Measures magnetic field for absolute heading"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"bno055-vs-mpu6050",children:"BNO055 vs. MPU6050"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Feature"}),(0,r.jsx)(n.th,{children:"BNO055"}),(0,r.jsx)(n.th,{children:"MPU6050"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Axes"})}),(0,r.jsx)(n.td,{children:"9-axis (Accel + Gyro + Mag)"}),(0,r.jsx)(n.td,{children:"6-axis (Accel + Gyro)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Sensor Fusion"})}),(0,r.jsxs)(n.td,{children:[(0,r.jsx)(n.strong,{children:"Hardware fusion"})," (ARM Cortex-M0)"]}),(0,r.jsx)(n.td,{children:"Software fusion required"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Output"})}),(0,r.jsx)(n.td,{children:"Quaternions, Euler angles, vectors"}),(0,r.jsx)(n.td,{children:"Raw sensor data"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Ease of Use"})}),(0,r.jsx)(n.td,{children:"Plug-and-play for orientation"}),(0,r.jsx)(n.td,{children:"Requires complex software"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Price"})}),(0,r.jsx)(n.td,{children:"~$30"}),(0,r.jsx)(n.td,{children:"~$5"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Best For"})}),(0,r.jsx)(n.td,{children:"Drones, humanoids (absolute orientation)"}),(0,r.jsx)(n.td,{children:"Cost-sensitive projects"})]})]})]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"For this course"}),": The ",(0,r.jsx)(n.strong,{children:"BNO055"})," is recommended because it outputs fused orientation data directly, saving you from implementing complex sensor fusion algorithms."]}),"\n",(0,r.jsx)(n.h3,{id:"why-imus-matter-for-humanoids",children:"Why IMUs Matter for Humanoids"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Balance Control"}),": Detect when the robot is tilting and correct posture"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Odometry"}),": Estimate position by integrating acceleration (prone to drift)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Sensor Fusion"}),": Combine with cameras/LiDAR for robust localization"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"4-forcetorque-sensors",children:"4. Force/Torque Sensors"}),"\n",(0,r.jsx)(n.p,{children:"Force/torque sensors measure the forces and moments applied to a robot's joints or end-effector (gripper)."}),"\n",(0,r.jsx)(n.h3,{id:"applications",children:"Applications"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Grasping"}),": Detect when an object is securely held"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Compliance Control"}),": Apply precise forces (e.g., polishing, assembly)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Safety"}),": Detect collisions and stop movement"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Manipulation"}),": Adjust grip based on object weight and fragility"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"example-ati-mini40",children:"Example: ATI Mini40"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Force Range"}),": \xb140 N (all axes)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Torque Range"}),": \xb12 N\xb7m (all axes)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Resolution"}),": 0.02 N (force), 0.001 N\xb7m (torque)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Price"}),": ~$3,000"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Note"}),": Force/torque sensors are expensive and typically found only in high-end research robots. For this course, we'll simulate them in Gazebo."]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"5-sensor-fusion",children:"5. Sensor Fusion"}),"\n",(0,r.jsxs)(n.p,{children:["No single sensor is perfect. ",(0,r.jsx)(n.strong,{children:"Sensor fusion"})," combines data from multiple sensors to overcome individual limitations."]}),"\n",(0,r.jsx)(n.h3,{id:"example-visual-inertial-odometry-vio",children:"Example: Visual-Inertial Odometry (VIO)"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Problem"}),": Cameras struggle in low light; IMUs drift over time."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Solution"}),": Fuse camera and IMU data:"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Camera"}),": Provides visual features for position estimation"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"IMU"}),": Provides high-frequency motion updates"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Fusion"}),": Kalman filter combines both, compensating for each sensor's weaknesses"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Result"}),": Robust localization that works in varied lighting and doesn't drift."]}),"\n",(0,r.jsx)(n.h3,{id:"common-fusion-techniques",children:"Common Fusion Techniques"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Kalman Filter"}),": Optimal for linear systems with Gaussian noise"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Extended Kalman Filter (EKF)"}),": For non-linear systems"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Particle Filter"}),": For multi-modal distributions"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Graph-Based SLAM"}),": Optimizes sensor measurements over time"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"ros-2-code-example-reading-sensor-data",children:"ROS 2 Code Example: Reading Sensor Data"}),"\n",(0,r.jsx)(n.p,{children:"Let's write our first ROS 2 program to subscribe to IMU data. This previews Chapter 2's content."}),"\n",(0,r.jsx)(n.h3,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"ROS 2 Humble installed"}),"\n",(0,r.jsx)(n.li,{children:"Python 3.10+"}),"\n",(0,r.jsx)(n.li,{children:"A sensor publishing IMU data (or simulated data)"}),"\n"]}),"\n",(0,r.jsxs)(n.h3,{id:"code-imu_subscriberpy",children:["Code: ",(0,r.jsx)(n.code,{children:"imu_subscriber.py"})]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\r\n"""\r\nSimple ROS 2 subscriber to read IMU sensor data.\r\nDemonstrates basic sensor integration in Physical AI systems.\r\n"""\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import Imu\r\nimport math\r\n\r\nclass IMUSubscriber(Node):\r\n    """\r\n    Subscribes to IMU data and prints orientation in Euler angles.\r\n    """\r\n    \r\n    def __init__(self):\r\n        super().__init__(\'imu_subscriber\')\r\n        \r\n        # Create subscription to /imu/data topic\r\n        self.subscription = self.create_subscription(\r\n            Imu,\r\n            \'/imu/data\',\r\n            self.imu_callback,\r\n            10  # QoS queue size\r\n        )\r\n        self.get_logger().info(\'IMU Subscriber started. Listening to /imu/data...\')\r\n    \r\n    def imu_callback(self, msg: Imu):\r\n        """\r\n        Callback function executed when IMU data is received.\r\n        \r\n        Args:\r\n            msg: IMU message containing orientation, angular velocity, linear acceleration\r\n        """\r\n        # Extract quaternion orientation\r\n        q = msg.orientation\r\n        \r\n        # Convert quaternion to Euler angles (roll, pitch, yaw)\r\n        roll, pitch, yaw = self.quaternion_to_euler(q.x, q.y, q.z, q.w)\r\n        \r\n        # Extract angular velocity\r\n        angular_vel = msg.angular_velocity\r\n        \r\n        # Extract linear acceleration\r\n        linear_accel = msg.linear_acceleration\r\n        \r\n        # Log the data\r\n        self.get_logger().info(\r\n            f\'Orientation (deg): Roll={math.degrees(roll):.2f}, \'\r\n            f\'Pitch={math.degrees(pitch):.2f}, Yaw={math.degrees(yaw):.2f}\'\r\n        )\r\n        self.get_logger().info(\r\n            f\'Angular Velocity (rad/s): x={angular_vel.x:.3f}, \'\r\n            f\'y={angular_vel.y:.3f}, z={angular_vel.z:.3f}\'\r\n        )\r\n        self.get_logger().info(\r\n            f\'Linear Acceleration (m/s\xb2): x={linear_accel.x:.3f}, \'\r\n            f\'y={linear_accel.y:.3f}, z={linear_accel.z:.3f}\\n\'\r\n        )\r\n    \r\n    def quaternion_to_euler(self, x, y, z, w):\r\n        """\r\n        Convert quaternion to Euler angles (roll, pitch, yaw).\r\n        \r\n        Args:\r\n            x, y, z, w: Quaternion components\r\n            \r\n        Returns:\r\n            tuple: (roll, pitch, yaw) in radians\r\n        """\r\n        # Roll (x-axis rotation)\r\n        sinr_cosp = 2 * (w * x + y * z)\r\n        cosr_cosp = 1 - 2 * (x * x + y * y)\r\n        roll = math.atan2(sinr_cosp, cosr_cosp)\r\n        \r\n        # Pitch (y-axis rotation)\r\n        sinp = 2 * (w * y - z * x)\r\n        if abs(sinp) >= 1:\r\n            pitch = math.copysign(math.pi / 2, sinp)  # Use 90 degrees if out of range\r\n        else:\r\n            pitch = math.asin(sinp)\r\n        \r\n        # Yaw (z-axis rotation)\r\n        siny_cosp = 2 * (w * z + x * y)\r\n        cosy_cosp = 1 - 2 * (y * y + z * z)\r\n        yaw = math.atan2(siny_cosp, cosy_cosp)\r\n        \r\n        return roll, pitch, yaw\r\n\r\ndef main(args=None):\r\n    """\r\n    Main function to initialize and run the ROS 2 node.\r\n    """\r\n    rclpy.init(args=args)\r\n    \r\n    imu_subscriber = IMUSubscriber()\r\n    \r\n    try:\r\n        rclpy.spin(imu_subscriber)\r\n    except KeyboardInterrupt:\r\n        imu_subscriber.get_logger().info(\'Shutting down IMU Subscriber...\')\r\n    finally:\r\n        imu_subscriber.destroy_node()\r\n        rclpy.shutdown()\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n",(0,r.jsx)(n.h3,{id:"how-to-run",children:"How to Run"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'# Terminal 1: Run the subscriber\r\npython3 imu_subscriber.py\r\n\r\n# Terminal 2: Publish test IMU data (if no real sensor)\r\nros2 topic pub /imu/data sensor_msgs/msg/Imu "{\r\n  orientation: {x: 0.0, y: 0.0, z: 0.0, w: 1.0},\r\n  angular_velocity: {x: 0.1, y: 0.2, z: 0.3},\r\n  linear_acceleration: {x: 0.0, y: 0.0, z: 9.81}\r\n}"\n'})}),"\n",(0,r.jsx)(n.h3,{id:"code-explanation",children:"Code Explanation"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Import ROS 2 Libraries"}),": ",(0,r.jsx)(n.code,{children:"rclpy"})," (ROS Client Library for Python)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Create Node"}),": Inherit from ",(0,r.jsx)(n.code,{children:"Node"})," class"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Subscribe to Topic"}),": Listen to ",(0,r.jsx)(n.code,{children:"/imu/data"})," (standard IMU topic)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Callback Function"}),": Processes incoming messages"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Quaternion to Euler"}),": Converts orientation to human-readable angles"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Logging"}),": Prints data to console"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"What You'll Learn in Chapter 2"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"ROS 2 architecture and concepts"}),"\n",(0,r.jsx)(n.li,{children:"Publishers, subscribers, services, actions"}),"\n",(0,r.jsx)(n.li,{children:"Creating custom message types"}),"\n",(0,r.jsx)(n.li,{children:"Launch files and parameters"}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,r.jsxs)(n.p,{children:["\u2705 ",(0,r.jsx)(n.strong,{children:"LiDAR"})," provides precise 3D spatial data; Livox Mid-360 is cost-effective for education"]}),"\n",(0,r.jsxs)(n.p,{children:["\u2705 ",(0,r.jsx)(n.strong,{children:"RGB-D Cameras"})," (RealSense D435i) combine vision and depth for object recognition and manipulation"]}),"\n",(0,r.jsxs)(n.p,{children:["\u2705 ",(0,r.jsx)(n.strong,{children:"IMUs"})," (BNO055) provide orientation data essential for balance and navigation"]}),"\n",(0,r.jsxs)(n.p,{children:["\u2705 ",(0,r.jsx)(n.strong,{children:"Force/Torque Sensors"})," enable delicate manipulation but are expensive"]}),"\n",(0,r.jsxs)(n.p,{children:["\u2705 ",(0,r.jsx)(n.strong,{children:"Sensor Fusion"})," combines multiple sensors to overcome individual limitations"]}),"\n",(0,r.jsxs)(n.p,{children:["\u2705 ",(0,r.jsx)(n.strong,{children:"ROS 2"})," is the standard framework for integrating sensors in robotics"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"reflection-questions",children:"Reflection Questions"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Why might a robot use both LiDAR and cameras instead of just one?"}),"\n",(0,r.jsx)(n.li,{children:"What are the advantages of hardware sensor fusion (BNO055) vs. software fusion (MPU6050)?"}),"\n",(0,r.jsx)(n.li,{children:"In what scenarios would a solid-state LiDAR (Livox) be preferable to a mechanical spinning LiDAR (Velodyne)?"}),"\n",(0,r.jsx)(n.li,{children:"How does the RealSense D435i's integrated IMU improve its performance for robotics?"}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Intel RealSense Documentation"}),": ",(0,r.jsx)(n.a,{href:"https://www.intelrealsense.com",children:"intelrealsense.com"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Livox LiDAR Technology"}),": ",(0,r.jsx)(n.a,{href:"https://www.livoxtech.com",children:"livoxtech.com"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"BNO055 Datasheet"}),": Bosch Sensortec"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"ROS 2 Humble Documentation"}),": ",(0,r.jsx)(n.a,{href:"https://docs.ros.org/en/humble/",children:"docs.ros.org"})]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"chapter-1-summary",children:"Chapter 1 Summary"}),"\n",(0,r.jsx)(n.p,{children:"Congratulations! You've completed Chapter 1: Introduction to Physical AI. You now understand:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"The foundations of Physical AI and embodied intelligence"}),"\n",(0,r.jsx)(n.li,{children:"The transition from digital to physical AI and the sim-to-real gap"}),"\n",(0,r.jsx)(n.li,{children:"The current landscape of humanoid robotics platforms"}),"\n",(0,r.jsx)(n.li,{children:"The core sensor technologies that enable Physical AI"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Next"}),": In Chapter 2, we'll dive deep into ",(0,r.jsx)(n.strong,{children:"ROS 2"}),", the middleware that connects sensors, actuators, and AI algorithms into a cohesive robotic system."]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Previous Section"}),": ",(0,r.jsx)(n.a,{href:"/docs/physical-ai-book/intro/humanoid-landscape",children:"\u2190 1.3 Humanoid Robotics Landscape"}),(0,r.jsx)(n.br,{}),"\n",(0,r.jsx)(n.strong,{children:"Next Chapter"}),": ",(0,r.jsx)(n.a,{href:"/docs/physical-ai-book/chapter2/",children:"Chapter 2: ROS 2 Fundamentals \u2192"})]})]})}function h(e={}){const{wrapper:n}={...(0,l.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>t,x:()=>o});var i=s(6540);const r={},l=i.createContext(r);function t(e){const n=i.useContext(l);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:t(e.components),i.createElement(l.Provider,{value:n},e.children)}}}]);