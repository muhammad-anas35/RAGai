"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[7200],{1521:(n,o,e)=>{e.r(o),e.d(o,{assets:()=>a,contentTitle:()=>l,default:()=>d,frontMatter:()=>s,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"physical-ai-book/chapter5/locomotion-control","title":"5.1 Locomotion Control","description":"Learning Objectives","source":"@site/docs/physical-ai-book/chapter5/locomotion-control.md","sourceDirName":"physical-ai-book/chapter5","slug":"/physical-ai-book/chapter5/locomotion-control","permalink":"/RAGai/docs/physical-ai-book/chapter5/locomotion-control","draft":false,"unlisted":false,"editUrl":"https://github.com/muhammad-anas35/RAGai/tree/main/docs/physical-ai-book/chapter5/locomotion-control.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 5: Humanoid Robot Development","permalink":"/RAGai/docs/physical-ai-book/chapter5/"},"next":{"title":"5.2 Manipulation and Grasping","permalink":"/RAGai/docs/physical-ai-book/chapter5/manipulation-grasping"}}');var i=e(4848),t=e(8453);const s={},l="5.1 Locomotion Control",a={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction",id:"introduction",level:2},{value:"Zero Moment Point (ZMP)",id:"zero-moment-point-zmp",level:2},{value:"Model Predictive Control (MPC)",id:"model-predictive-control-mpc",level:2},{value:"RL-Based Locomotion",id:"rl-based-locomotion",level:2},{value:"Deployment Example",id:"deployment-example",level:2},{value:"Key Takeaways",id:"key-takeaways",level:2}];function p(n){const o={a:"a",br:"br",code:"code",h1:"h1",h2:"h2",header:"header",hr:"hr",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...n.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(o.header,{children:(0,i.jsx)(o.h1,{id:"51-locomotion-control",children:"5.1 Locomotion Control"})}),"\n",(0,i.jsx)(o.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,i.jsxs)(o.ul,{children:["\n",(0,i.jsx)(o.li,{children:"Understand bipedal walking dynamics"}),"\n",(0,i.jsx)(o.li,{children:"Implement Zero Moment Point (ZMP) control"}),"\n",(0,i.jsx)(o.li,{children:"Use Model Predictive Control (MPC) for locomotion"}),"\n",(0,i.jsx)(o.li,{children:"Train RL-based walking policies"}),"\n",(0,i.jsx)(o.li,{children:"Deploy locomotion controllers on real humanoids"}),"\n"]}),"\n",(0,i.jsx)(o.hr,{}),"\n",(0,i.jsx)(o.h2,{id:"introduction",children:"Introduction"}),"\n",(0,i.jsxs)(o.p,{children:[(0,i.jsx)(o.strong,{children:"Locomotion"})," is the ability to move through an environment. For humanoid robots, bipedal walking is one of the most challenging control problems due to:"]}),"\n",(0,i.jsxs)(o.ul,{children:["\n",(0,i.jsxs)(o.li,{children:[(0,i.jsx)(o.strong,{children:"Underactuation"}),": Fewer actuators than degrees of freedom"]}),"\n",(0,i.jsxs)(o.li,{children:[(0,i.jsx)(o.strong,{children:"Contact dynamics"}),": Complex foot-ground interactions"]}),"\n",(0,i.jsxs)(o.li,{children:[(0,i.jsx)(o.strong,{children:"Balance"}),": Maintaining stability while moving"]}),"\n"]}),"\n",(0,i.jsx)(o.hr,{}),"\n",(0,i.jsx)(o.h2,{id:"zero-moment-point-zmp",children:"Zero Moment Point (ZMP)"}),"\n",(0,i.jsxs)(o.p,{children:[(0,i.jsx)(o.strong,{children:"ZMP"})," is a point on the ground where the net moment from contact forces is zero. For stable walking:"]}),"\n",(0,i.jsxs)(o.ul,{children:["\n",(0,i.jsxs)(o.li,{children:["ZMP must stay ",(0,i.jsx)(o.strong,{children:"inside the support polygon"})," (foot contact area)"]}),"\n"]}),"\n",(0,i.jsx)(o.pre,{children:(0,i.jsx)(o.code,{className:"language-python",children:'def compute_zmp(com_pos, com_acc, gravity=9.81):\r\n    """\r\n    Compute ZMP from center of mass (CoM) state\r\n    """\r\n    zmp_x = com_pos[0] - (com_pos[2] / gravity) * com_acc[0]\r\n    zmp_y = com_pos[1] - (com_pos[2] / gravity) * com_acc[1]\r\n    return [zmp_x, zmp_y]\r\n\r\n# Check stability\r\ndef is_stable(zmp, support_polygon):\r\n    return point_in_polygon(zmp, support_polygon)\n'})}),"\n",(0,i.jsx)(o.hr,{}),"\n",(0,i.jsx)(o.h2,{id:"model-predictive-control-mpc",children:"Model Predictive Control (MPC)"}),"\n",(0,i.jsx)(o.p,{children:"MPC plans future trajectories by solving an optimization problem:"}),"\n",(0,i.jsx)(o.pre,{children:(0,i.jsx)(o.code,{className:"language-python",children:"import casadi as ca\r\n\r\n# Define optimization problem\r\nopti = ca.Opti()\r\n\r\n# Decision variables (foot positions over horizon)\r\nN = 20  # Horizon steps\r\nfoot_pos = opti.variable(N, 3)\r\n\r\n# Objective: minimize CoM tracking error\r\ncom_ref = [0, 0, 0.9]  # Desired CoM height\r\ncost = ca.sumsqr(com_pos - com_ref)\r\n\r\n# Constraints: ZMP stability\r\nfor k in range(N):\r\n    zmp = compute_zmp(com_pos[k], com_acc[k])\r\n    opti.subject_to(zmp_in_support(zmp, foot_pos[k]))\r\n\r\n# Solve\r\nopti.minimize(cost)\r\nsol = opti.solve()\n"})}),"\n",(0,i.jsx)(o.hr,{}),"\n",(0,i.jsx)(o.h2,{id:"rl-based-locomotion",children:"RL-Based Locomotion"}),"\n",(0,i.jsx)(o.p,{children:"Train walking policies with Isaac Gym:"}),"\n",(0,i.jsx)(o.pre,{children:(0,i.jsx)(o.code,{className:"language-python",children:"class HumanoidWalkEnv:\r\n    def compute_reward(self):\r\n        # Forward velocity reward\r\n        vel_reward = self.base_lin_vel[0]\r\n        \r\n        # Upright orientation reward\r\n        up_reward = torch.sum(self.base_quat[:, 2])\r\n        \r\n        # Energy penalty\r\n        energy_penalty = -0.01 * torch.sum(self.dof_vel ** 2)\r\n        \r\n        return vel_reward + up_reward + energy_penalty\n"})}),"\n",(0,i.jsxs)(o.p,{children:[(0,i.jsx)(o.strong,{children:"Training"}),":"]}),"\n",(0,i.jsxs)(o.ul,{children:["\n",(0,i.jsx)(o.li,{children:"2048 parallel environments"}),"\n",(0,i.jsx)(o.li,{children:"PPO algorithm"}),"\n",(0,i.jsx)(o.li,{children:"10M timesteps (~2 hours on RTX 4090)"}),"\n"]}),"\n",(0,i.jsx)(o.hr,{}),"\n",(0,i.jsx)(o.h2,{id:"deployment-example",children:"Deployment Example"}),"\n",(0,i.jsx)(o.pre,{children:(0,i.jsx)(o.code,{className:"language-python",children:"# Load trained policy\r\npolicy = torch.load(\"humanoid_walk.pth\")\r\n\r\n# ROS 2 node for real robot\r\nclass LocomotionController(Node):\r\n    def __init__(self):\r\n        super().__init__('locomotion_controller')\r\n        self.joint_pub = self.create_publisher(\r\n            JointTrajectory, '/joint_commands', 10\r\n        )\r\n        \r\n    def control_loop(self, obs):\r\n        with torch.no_grad():\r\n            actions = policy(obs)\r\n        \r\n        # Send to robot\r\n        msg = JointTrajectory()\r\n        msg.points = [JointTrajectoryPoint(positions=actions.tolist())]\r\n        self.joint_pub.publish(msg)\n"})}),"\n",(0,i.jsx)(o.hr,{}),"\n",(0,i.jsx)(o.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,i.jsxs)(o.p,{children:["\u2705 ",(0,i.jsx)(o.strong,{children:"ZMP"})," ensures stability by keeping balance point in support polygon",(0,i.jsx)(o.br,{}),"\n","\u2705 ",(0,i.jsx)(o.strong,{children:"MPC"})," plans optimal trajectories with constraints",(0,i.jsx)(o.br,{}),"\n","\u2705 ",(0,i.jsx)(o.strong,{children:"RL"})," learns robust policies from simulation",(0,i.jsx)(o.br,{}),"\n","\u2705 ",(0,i.jsx)(o.strong,{children:"Sim-to-real"})," transfer requires domain randomization"]}),"\n",(0,i.jsx)(o.hr,{}),"\n",(0,i.jsxs)(o.p,{children:[(0,i.jsx)(o.strong,{children:"Previous Chapter"}),": ",(0,i.jsx)(o.a,{href:"/RAGai/docs/physical-ai-book/chapter4/isaac-ros2-bridge",children:"\u2190 Chapter 4: NVIDIA Isaac"}),(0,i.jsx)(o.br,{}),"\n",(0,i.jsx)(o.strong,{children:"Next Section"}),": ",(0,i.jsx)(o.a,{href:"/RAGai/docs/physical-ai-book/chapter5/manipulation-grasping",children:"5.2 Manipulation and Grasping \u2192"})]})]})}function d(n={}){const{wrapper:o}={...(0,t.R)(),...n.components};return o?(0,i.jsx)(o,{...n,children:(0,i.jsx)(p,{...n})}):p(n)}},8453:(n,o,e)=>{e.d(o,{R:()=>s,x:()=>l});var r=e(6540);const i={},t=r.createContext(i);function s(n){const o=r.useContext(t);return r.useMemo(function(){return"function"==typeof n?n(o):{...o,...n}},[o,n])}function l(n){let o;return o=n.disableParentContext?"function"==typeof n.components?n.components(i):n.components||i:s(n.components),r.createElement(t.Provider,{value:o},n.children)}}}]);