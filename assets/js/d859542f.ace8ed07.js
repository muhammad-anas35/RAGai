"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[9858],{8453:(e,n,r)=>{r.d(n,{R:()=>t,x:()=>a});var o=r(6540);const i={},s=o.createContext(i);function t(e){const n=o.useContext(s);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:t(e.components),o.createElement(s.Provider,{value:n},e.children)}},8743:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>t,metadata:()=>o,toc:()=>c});const o=JSON.parse('{"id":"physical-ai-book/chapter6/multimodal-integration","title":"6.2 Multimodal Integration","description":"Learning Objectives","source":"@site/docs/physical-ai-book/chapter6/multimodal-integration.md","sourceDirName":"physical-ai-book/chapter6","slug":"/physical-ai-book/chapter6/multimodal-integration","permalink":"/docs/physical-ai-book/chapter6/multimodal-integration","draft":false,"unlisted":false,"editUrl":"https://github.com/muhammad-anas35/RAGai/tree/main/docs/physical-ai-book/chapter6/multimodal-integration.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 6: Conversational Robotics","permalink":"/docs/physical-ai-book/chapter6/"},"next":{"title":"6.1 Vision-Language-Action (VLA) Foundations","permalink":"/docs/physical-ai-book/chapter6/vla-foundations"}}');var i=r(4848),s=r(8453);const t={},a="6.2 Multimodal Integration",l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction",id:"introduction",level:2},{value:"Multimodal Observation Space",id:"multimodal-observation-space",level:2},{value:"Temporal Integration",id:"temporal-integration",level:2},{value:"Sensor Fusion",id:"sensor-fusion",level:2},{value:"Audio Integration",id:"audio-integration",level:2},{value:"Key Takeaways",id:"key-takeaways",level:2}];function d(e){const n={a:"a",br:"br",code:"code",h1:"h1",h2:"h2",header:"header",hr:"hr",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"62-multimodal-integration",children:"6.2 Multimodal Integration"})}),"\n",(0,i.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Integrate vision, language, and proprioception"}),"\n",(0,i.jsx)(n.li,{children:"Implement sensor fusion for VLAs"}),"\n",(0,i.jsx)(n.li,{children:"Handle temporal information (video, history)"}),"\n",(0,i.jsx)(n.li,{children:"Build multimodal observation spaces"}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Multimodal integration"})," combines multiple sensor modalities:"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Vision"}),": RGB, depth, segmentation"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Language"}),": Commands, descriptions, feedback"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Proprioception"}),": Joint positions, velocities, forces"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Audio"}),": Speech, environmental sounds"]}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"multimodal-observation-space",children:"Multimodal Observation Space"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"class MultimodalObservation:\r\n    def __init__(self):\r\n        self.rgb = None          # (H, W, 3)\r\n        self.depth = None        # (H, W, 1)\r\n        self.language = None     # String\r\n        self.joint_pos = None    # (n_joints,)\r\n        self.joint_vel = None    # (n_joints,)\r\n        self.gripper_force = None  # Scalar\r\n        \r\n    def to_tensor(self):\r\n        # Encode vision\r\n        vision_features = vision_encoder(self.rgb, self.depth)\r\n        \r\n        # Encode language\r\n        lang_features = language_encoder(self.language)\r\n        \r\n        # Concatenate all modalities\r\n        obs = torch.cat([\r\n            vision_features,\r\n            lang_features,\r\n            self.joint_pos,\r\n            self.joint_vel,\r\n            self.gripper_force\r\n        ])\r\n        \r\n        return obs\n"})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"temporal-integration",children:"Temporal Integration"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"class TemporalVLA:\r\n    def __init__(self, history_length=10):\r\n        self.history = deque(maxlen=history_length)\r\n        \r\n    def predict(self, current_obs):\r\n        # Add to history\r\n        self.history.append(current_obs)\r\n        \r\n        # Stack history\r\n        obs_sequence = torch.stack(list(self.history))\r\n        \r\n        # Temporal encoding (LSTM or Transformer)\r\n        temporal_features = self.temporal_encoder(obs_sequence)\r\n        \r\n        # Predict action\r\n        action = self.policy(temporal_features)\r\n        \r\n        return action\n"})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"sensor-fusion",children:"Sensor Fusion"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class SensorFusion:\r\n    def fuse_depth_sources(self, stereo_depth, lidar_depth):\r\n        """\r\n        Fuse stereo camera depth with LiDAR depth\r\n        """\r\n        # Kalman filter fusion\r\n        fused_depth = self.kalman_filter.update(\r\n            measurement_1=stereo_depth,\r\n            measurement_2=lidar_depth,\r\n            covariance_1=stereo_cov,\r\n            covariance_2=lidar_cov\r\n        )\r\n        \r\n        return fused_depth\n'})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"audio-integration",children:"Audio Integration"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from transformers import WhisperProcessor, WhisperForConditionalGeneration\r\n\r\n# Speech-to-text\r\nprocessor = WhisperProcessor.from_pretrained("openai/whisper-base")\r\nmodel = WhisperForConditionalGeneration.from_pretrained("openai/whisper-base")\r\n\r\n# Process audio\r\naudio = microphone.get_audio()\r\ninputs = processor(audio, return_tensors="pt", sampling_rate=16000)\r\npredicted_ids = model.generate(inputs.input_features)\r\n\r\n# Decode to text\r\ntranscription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\r\n\r\n# Use as language input\r\naction = vla_model.predict(\r\n    image=camera.get_rgb(),\r\n    instruction=transcription,\r\n    robot_state=robot.get_state()\r\n)\n'})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,i.jsxs)(n.p,{children:["\u2705 ",(0,i.jsx)(n.strong,{children:"Multimodal"})," observations improve robustness",(0,i.jsx)(n.br,{}),"\n","\u2705 ",(0,i.jsx)(n.strong,{children:"Temporal"})," information captures dynamics",(0,i.jsx)(n.br,{}),"\n","\u2705 ",(0,i.jsx)(n.strong,{children:"Sensor fusion"})," combines complementary modalities",(0,i.jsx)(n.br,{}),"\n","\u2705 ",(0,i.jsx)(n.strong,{children:"Audio"})," enables voice control"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Previous Section"}),": ",(0,i.jsx)(n.a,{href:"/docs/physical-ai-book/chapter6/vla-foundations",children:"\u2190 6.1 VLA Foundations"}),(0,i.jsx)(n.br,{}),"\n",(0,i.jsx)(n.strong,{children:"Next Section"}),": ",(0,i.jsx)(n.a,{href:"/docs/physical-ai-book/chapter6/deployment-strategies",children:"6.3 Deployment Strategies \u2192"})]})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}}}]);